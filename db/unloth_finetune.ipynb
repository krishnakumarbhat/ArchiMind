{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25ec055",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Trainer' from 'unsloth' (/home/pope/.local/lib/python3.10/site-packages/unsloth/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Remove '==4.41.1' from transformers to let Unsloth choose the right version\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# !pip install -q unsloth==2025.10.1 'accelerate>=0.30.1' bitsandbytes datasets peft transformers\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel, Trainer, TrainingArguments, SFTTrainer, get_peft_model\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Trainer' from 'unsloth' (/home/pope/.local/lib/python3.10/site-packages/unsloth/__init__.py)"
     ]
    }
   ],
   "source": [
    "# !pip install -q unsloth==0.6.5 accelerate>=0.30.1 bitsandbytes datasets peft transformers==4.41.1\n",
    "# !pip install -q unsloth==0.6.5 'accelerate>=0.30.1' bitsandbytes datasets peft transformers==4.41.1\n",
    "# !pip install -q unsloth==2025.10.1 'accelerate>=0.30.1' bitsandbytes datasets peft transformers==4.41.1\n",
    "import os\n",
    "# Remove '==4.41.1' from transformers to let Unsloth choose the right version\n",
    "# !pip install -q unsloth==2025.10.1 'accelerate>=0.30.1' bitsandbytes datasets peft transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel, Trainer, TrainingArguments, SFTTrainer, get_peft_model\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41daa4d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SFTTrainer' from 'unsloth' (/home/pope/.local/lib/python3.10/site-packages/unsloth/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Corrected imports\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel, SFTTrainer, get_peft_model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, TrainingArguments\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SFTTrainer' from 'unsloth' (/home/pope/.local/lib/python3.10/site-packages/unsloth/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Corrected imports\n",
    "from unsloth import FastLanguageModel, SFTTrainer, get_peft_model\n",
    "from transformers import AutoTokenizer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12145b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b52a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen1.5-7B-Chat\"\n",
    "OUTPUT_DIR = \"outputs/qwen-finetuned\"\n",
    "DATA_PATH = \"dataset_to train.json\"  # update to actual file\n",
    "TEXT_FIELD = \"text\"                       # update if column name differs\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "WARMUP_RATIO = 0.03\n",
    "SAVE_STEPS = 500\n",
    "torch_dtype = \"bfloat16\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e2bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=DATA_PATH)\n",
    "train_dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=torch_dtype,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e200a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    fp16=False,\n",
    "    bf16=torch_dtype == \"bfloat16\",\n",
    "    logging_steps=10,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    evaluation_strategy=\"no\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c3b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    dataset_text_field=TEXT_FIELD,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    args=training_args,\n",
    "    packing=True,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce268ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.merge_lora(model, tokenizer)\n",
    "model.save_pretrained(OUTPUT_DIR, safe_serialization=False)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=OUTPUT_DIR, tokenizer=OUTPUT_DIR, max_new_tokens=256)\n",
    "prompt = \"### Instruction:\\nExplain the benefits of knowledge graphs.\\n\\n### Response:\"\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882bb727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
