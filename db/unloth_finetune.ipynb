{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25ec055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement unsloth==0.6.5 (from versions: 2024.8, 2024.9, 2024.9.post1, 2024.9.post2, 2024.9.post3, 2024.9.post4, 2024.10.0, 2024.10.1, 2024.10.2, 2024.10.4, 2024.10.5, 2024.10.6, 2024.10.7, 2024.11.2, 2024.11.4, 2024.11.5, 2024.11.6, 2024.11.7, 2024.11.8, 2024.11.9, 2024.11.10, 2024.11.11, 2024.12.1, 2024.12.2, 2024.12.3, 2024.12.4, 2024.12.5, 2024.12.6, 2024.12.7, 2024.12.8, 2024.12.9, 2024.12.10, 2024.12.11, 2024.12.12, 2025.1.1, 2025.1.2, 2025.1.3, 2025.1.4, 2025.1.5, 2025.1.6, 2025.1.8, 2025.2.2, 2025.2.3, 2025.2.4, 2025.2.5, 2025.2.6, 2025.2.7, 2025.2.8, 2025.2.9, 2025.2.10, 2025.2.11, 2025.2.12, 2025.2.13, 2025.2.14, 2025.2.15, 2025.3.1, 2025.3.2, 2025.3.3, 2025.3.4, 2025.3.5, 2025.3.6, 2025.3.7, 2025.3.8, 2025.3.9, 2025.3.10, 2025.3.11, 2025.3.12, 2025.3.13, 2025.3.14, 2025.3.15, 2025.3.16, 2025.3.17, 2025.3.18, 2025.3.19, 2025.4.1, 2025.4.2, 2025.4.3, 2025.4.4, 2025.4.5, 2025.4.7, 2025.5.1, 2025.5.2, 2025.5.3, 2025.5.4, 2025.5.5, 2025.5.6, 2025.5.7, 2025.5.8, 2025.5.9, 2025.6.1, 2025.6.2, 2025.6.3, 2025.6.4, 2025.6.5, 2025.6.6, 2025.6.7, 2025.6.8, 2025.6.9, 2025.6.10, 2025.6.11, 2025.6.12, 2025.7.1, 2025.7.2, 2025.7.3, 2025.7.4, 2025.7.5, 2025.7.6, 2025.7.7, 2025.7.8, 2025.7.10, 2025.7.11, 2025.8.1, 2025.8.2, 2025.8.3, 2025.8.4, 2025.8.5, 2025.8.6, 2025.8.7, 2025.8.8, 2025.8.9, 2025.8.10, 2025.9.1, 2025.9.2, 2025.9.3, 2025.9.4, 2025.9.5, 2025.9.6, 2025.9.7, 2025.9.8, 2025.9.9, 2025.9.10, 2025.9.11, 2025.10.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for unsloth==0.6.5\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip install -q unsloth==0.6.5 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=0.30.1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m bitsandbytes datasets peft transformers==4.41.1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel, Trainer, TrainingArguments, SFTTrainer, get_peft_model\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "# !pip install -q unsloth==0.6.5 accelerate>=0.30.1 bitsandbytes datasets peft transformers==4.41.1\n",
    "!pip install -q unsloth==0.6.5 'accelerate>=0.30.1' bitsandbytes datasets peft transformers==4.41.1\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel, Trainer, TrainingArguments, SFTTrainer, get_peft_model\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b52a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen1.5-7B-Chat\"\n",
    "OUTPUT_DIR = \"outputs/qwen-finetuned\"\n",
    "DATA_PATH = \"path/to/your_dataset.jsonl\"  # update to actual file\n",
    "TEXT_FIELD = \"text\"                       # update if column name differs\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "WARMUP_RATIO = 0.03\n",
    "SAVE_STEPS = 500\n",
    "torch_dtype = \"bfloat16\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e2bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"json\", data_files=DATA_PATH)\n",
    "train_dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84f2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=torch_dtype,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e200a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    fp16=False,\n",
    "    bf16=torch_dtype == \"bfloat16\",\n",
    "    logging_steps=10,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    evaluation_strategy=\"no\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c3b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    dataset_text_field=TEXT_FIELD,\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    args=training_args,\n",
    "    packing=True,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce268ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.merge_lora(model, tokenizer)\n",
    "model.save_pretrained(OUTPUT_DIR, safe_serialization=False)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=OUTPUT_DIR, tokenizer=OUTPUT_DIR, max_new_tokens=256)\n",
    "prompt = \"### Instruction:\\nExplain the benefits of knowledge graphs.\\n\\n### Response:\"\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882bb727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
